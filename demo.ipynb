{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T11:28:55.179905Z",
     "start_time": "2025-04-21T11:28:55.176520Z"
    }
   },
   "source": [
    "# Prepare the Environment\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:05:49.808675Z",
     "start_time": "2025-04-21T11:05:48.664270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup Hyperparameters\n",
    "# Hyperparameters\n",
    "batch_size = 4  # How many batches per training step\n",
    "context_length = 16  # Length of the token chunk each batch\n",
    "d_model = 64  # The vector size of the token embeddings\n",
    "num_layers = 8  # Number of transformer blocks\n",
    "num_heads = 4  # Number of heads in Multi-head attention # 我们的代码中通过 d_model / num_heads = 来获取 head_size\n",
    "learning_rate = 1e-3  # 0.001\n",
    "dropout = 0.1 # Dropout rate\n",
    "max_iters = 5000  # Total of training iterations\n",
    "eval_interval = 50  # How often to evaluate the model\n",
    "eval_iters = 20  # How many iterations to average the loss over when evaluating the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Instead of using the cpu, we'll use the GPU if it's available.\n",
    "\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)"
   ],
   "id": "afd4e02703444b73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x272791ddd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:05:54.669076Z",
     "start_time": "2025-04-21T11:05:54.661650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the Dataset\n",
    "# download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "id": "6d0bfd1654e21f29",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:09:23.441940Z",
     "start_time": "2025-04-21T11:09:22.608725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Tokenization\n",
    "# Using TikToken to tokenize the source text\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = torch.tensor(encoding.encode(text)) # size of tokenized source text is 77,919\n",
    "vocab_size = len(set(tokenized_text)) # size of vocabulary is 3,771\n",
    "max_token_value = max(tokenized_text)\n",
    "\n",
    "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"The maximum value in the tokenized text is: {max_token_value}\")"
   ],
   "id": "e9eef85cc0b4b539",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text size: 77919\n",
      "Vocabulary size: 77919\n",
      "The maximum value in the tokenized text is: 100069\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:09:25.836171Z",
     "start_time": "2025-04-21T11:09:25.829257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Word Embedding\n",
    "# Split train and validation\n",
    "split_idx = int(len(tokenized_text) * 0.8)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]\n",
    "\n",
    "# Prepare data for training batch\n",
    "# Prepare data for training batch\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs])\n",
    "print(x_batch.shape, x_batch.shape)"
   ],
   "id": "ce93ae375dae2a63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:39:26.811974Z",
     "start_time": "2025-04-21T12:39:26.724465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Positional Encoding\n",
    "# Define Token Embedding look-up table\n",
    "token_embedding_lookup_table = nn.Embedding(max_token_value, d_model)\n",
    "\n",
    "# Get X and Y embedding\n",
    "x = token_embedding_lookup_table(x_batch.data)\n",
    "y = token_embedding_lookup_table(y_batch.data)\n",
    "\n",
    "# Define Position Encoding look-up table\n",
    "position_encoding_lookup_table = torch.zeros(context_length, d_model) # initial with zeros with shape (context_length, d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "# apply the sine & cosine\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) #add batch to the first dimension\n",
    "\n",
    "print(\"Position Encoding Look-up Table: \", position_encoding_lookup_table.shape)\n",
    "\n",
    "# Add positional encoding into the input embedding vector\n",
    "input_embedding_x = x + position_encoding_lookup_table # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "input_embedding_y = y + position_encoding_lookup_table\n",
    "\n",
    "X = input_embedding_x\n",
    "\n",
    "x_plot = input_embedding_x[0].detach().cpu().numpy()\n",
    "print(\"Final Input Embedding of x: \\n\")\n",
    "pd.DataFrame(x_plot)"
   ],
   "id": "e697fc0207eef752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Encoding Look-up Table:  torch.Size([4, 16, 64])\n",
      "Final Input Embedding of x: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.283404  0.338045 -1.433035  2.767594  0.576844  1.222869  1.151606   \n",
       "1   2.405423  1.134329 -0.297423  1.513847 -0.094999  1.010294  0.945422   \n",
       "2   0.587248 -1.585031  0.868417  0.717200  0.750380  0.331701  0.446977   \n",
       "3   1.825541  0.508325  3.189547 -3.090334 -0.790009 -0.605251  3.264065   \n",
       "4  -1.716024 -1.586166  0.198546 -0.353335  3.309299  0.301478  0.728683   \n",
       "5  -1.650472  0.311326 -0.700890 -0.460482  1.144396  0.241577 -0.143885   \n",
       "6  -0.176708  1.625126 -3.843431  0.226879 -0.763457 -0.964958  1.278523   \n",
       "7   0.729852  2.036257 -1.206615  2.121351 -1.688091  0.052425 -0.156384   \n",
       "8   2.357656  0.037400  0.325845  1.146377 -0.941756  0.425242  0.890695   \n",
       "9   1.341450 -0.485298  1.107402  1.558929 -0.446929 -1.098254 -0.614124   \n",
       "10 -1.979038 -1.094407  3.389443  0.096807  0.548192  1.567318 -1.370609   \n",
       "11 -1.624343 -0.447252  1.903704 -0.778280 -0.256622  2.204152 -0.684100   \n",
       "12 -0.632599  1.512969  0.063073  0.599132  1.569201  1.063324 -1.061380   \n",
       "13  0.002009  0.888098 -0.023086 -1.635177  1.314313  2.097363  0.639267   \n",
       "14 -1.039065 -1.111172 -0.568909  0.952261  0.981354  0.272422 -2.143591   \n",
       "15 -0.707917 -0.256485 -0.452564 -0.123952  1.131047 -0.965086 -0.273190   \n",
       "\n",
       "          7         8         9   ...        54        55        56        57  \\\n",
       "0   1.042083  1.343139 -0.339763  ...  0.486147  1.545294  0.225854 -0.347028   \n",
       "1  -0.156378  0.798628 -0.492561  ...  0.451860  0.310261 -1.565642 -1.741235   \n",
       "2   1.132623  0.627753  0.099642  ... -2.053629  0.528360  0.559716 -0.131694   \n",
       "3  -0.547575  1.291236  2.863118  ... -1.765736  1.504601  0.736996  3.704933   \n",
       "4   1.458770  2.398213  0.092539  ...  0.739426  2.699533 -0.676643  0.467648   \n",
       "5   0.376465  0.179648 -0.371582  ...  1.243632 -0.405581  1.085325  1.973006   \n",
       "6  -0.875336 -0.665511 -0.101888  ...  0.091525  0.851619  0.267266  1.929255   \n",
       "7  -1.596992  1.558975  0.324975  ...  0.383543  0.338034 -0.095333 -0.785435   \n",
       "8  -1.161934  0.509555  0.397377  ...  0.773701  2.106979  0.112369  1.145097   \n",
       "9   0.109389  3.285658 -2.086398  ...  1.565137  0.503009 -0.716071  1.805186   \n",
       "10 -0.931647 -0.172467 -2.496654  ...  0.473266  0.977193 -0.845596  0.515236   \n",
       "11  0.221853  0.757391 -1.835602  ... -0.036426  2.565432 -0.404975  0.285705   \n",
       "12  0.315820 -0.638413  0.355060  ...  1.728565  1.498970  0.054502 -0.056662   \n",
       "13 -0.846642 -1.997691 -0.835741  ... -2.691700  3.085684  0.343530  2.113288   \n",
       "14  0.038883  0.340247 -0.874415  ...  0.038609  2.260000 -0.459777 -0.185047   \n",
       "15 -0.659415 -1.277485 -0.328948  ... -0.312385  1.883555  1.398873  1.191182   \n",
       "\n",
       "          58        59        60        61        62        63  \n",
       "0  -0.645690  2.254384  0.796207  1.492581  2.124295  1.222536  \n",
       "1   1.525025 -1.283047  0.545836  2.580286  1.007957  2.450777  \n",
       "2   0.353959  0.769088 -1.257144  2.869510  0.556096  0.200955  \n",
       "3  -0.113013  0.826981  0.393594  1.762055  0.384672  3.776245  \n",
       "4   0.635566  1.664962 -0.759751  1.407433 -0.478800  1.408918  \n",
       "5   0.290641  0.963295 -1.359209  2.193898  0.814181  1.235836  \n",
       "6  -0.984592  1.801168 -0.403364  0.300297 -1.973638  0.862498  \n",
       "7   1.885006  0.938034 -0.616108 -0.222979  1.323855  0.622632  \n",
       "8  -0.666539  1.018525  0.091615  1.301296 -0.844504  0.503519  \n",
       "9  -0.334426  0.404759  0.504305  2.839343  0.346494  3.016829  \n",
       "10  0.673198  0.404918 -0.761746 -0.420401  0.307847 -0.302959  \n",
       "11 -0.626100  0.045816 -1.684269  2.971559 -0.948307  0.275110  \n",
       "12  0.047258  0.418277 -2.493387  1.780422  1.000546  0.551793  \n",
       "13  1.427790  1.250673  0.566318  2.922274  0.636758  1.900987  \n",
       "14 -0.115265 -0.171016  1.042122  2.906338 -0.792142  0.486340  \n",
       "15 -0.404498  1.748411  0.581175  0.899900 -0.658613  1.504646  \n",
       "\n",
       "[16 rows x 64 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.283404</td>\n",
       "      <td>0.338045</td>\n",
       "      <td>-1.433035</td>\n",
       "      <td>2.767594</td>\n",
       "      <td>0.576844</td>\n",
       "      <td>1.222869</td>\n",
       "      <td>1.151606</td>\n",
       "      <td>1.042083</td>\n",
       "      <td>1.343139</td>\n",
       "      <td>-0.339763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486147</td>\n",
       "      <td>1.545294</td>\n",
       "      <td>0.225854</td>\n",
       "      <td>-0.347028</td>\n",
       "      <td>-0.645690</td>\n",
       "      <td>2.254384</td>\n",
       "      <td>0.796207</td>\n",
       "      <td>1.492581</td>\n",
       "      <td>2.124295</td>\n",
       "      <td>1.222536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.405423</td>\n",
       "      <td>1.134329</td>\n",
       "      <td>-0.297423</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>-0.094999</td>\n",
       "      <td>1.010294</td>\n",
       "      <td>0.945422</td>\n",
       "      <td>-0.156378</td>\n",
       "      <td>0.798628</td>\n",
       "      <td>-0.492561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451860</td>\n",
       "      <td>0.310261</td>\n",
       "      <td>-1.565642</td>\n",
       "      <td>-1.741235</td>\n",
       "      <td>1.525025</td>\n",
       "      <td>-1.283047</td>\n",
       "      <td>0.545836</td>\n",
       "      <td>2.580286</td>\n",
       "      <td>1.007957</td>\n",
       "      <td>2.450777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.587248</td>\n",
       "      <td>-1.585031</td>\n",
       "      <td>0.868417</td>\n",
       "      <td>0.717200</td>\n",
       "      <td>0.750380</td>\n",
       "      <td>0.331701</td>\n",
       "      <td>0.446977</td>\n",
       "      <td>1.132623</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>0.099642</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.053629</td>\n",
       "      <td>0.528360</td>\n",
       "      <td>0.559716</td>\n",
       "      <td>-0.131694</td>\n",
       "      <td>0.353959</td>\n",
       "      <td>0.769088</td>\n",
       "      <td>-1.257144</td>\n",
       "      <td>2.869510</td>\n",
       "      <td>0.556096</td>\n",
       "      <td>0.200955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.825541</td>\n",
       "      <td>0.508325</td>\n",
       "      <td>3.189547</td>\n",
       "      <td>-3.090334</td>\n",
       "      <td>-0.790009</td>\n",
       "      <td>-0.605251</td>\n",
       "      <td>3.264065</td>\n",
       "      <td>-0.547575</td>\n",
       "      <td>1.291236</td>\n",
       "      <td>2.863118</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.765736</td>\n",
       "      <td>1.504601</td>\n",
       "      <td>0.736996</td>\n",
       "      <td>3.704933</td>\n",
       "      <td>-0.113013</td>\n",
       "      <td>0.826981</td>\n",
       "      <td>0.393594</td>\n",
       "      <td>1.762055</td>\n",
       "      <td>0.384672</td>\n",
       "      <td>3.776245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.716024</td>\n",
       "      <td>-1.586166</td>\n",
       "      <td>0.198546</td>\n",
       "      <td>-0.353335</td>\n",
       "      <td>3.309299</td>\n",
       "      <td>0.301478</td>\n",
       "      <td>0.728683</td>\n",
       "      <td>1.458770</td>\n",
       "      <td>2.398213</td>\n",
       "      <td>0.092539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739426</td>\n",
       "      <td>2.699533</td>\n",
       "      <td>-0.676643</td>\n",
       "      <td>0.467648</td>\n",
       "      <td>0.635566</td>\n",
       "      <td>1.664962</td>\n",
       "      <td>-0.759751</td>\n",
       "      <td>1.407433</td>\n",
       "      <td>-0.478800</td>\n",
       "      <td>1.408918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.650472</td>\n",
       "      <td>0.311326</td>\n",
       "      <td>-0.700890</td>\n",
       "      <td>-0.460482</td>\n",
       "      <td>1.144396</td>\n",
       "      <td>0.241577</td>\n",
       "      <td>-0.143885</td>\n",
       "      <td>0.376465</td>\n",
       "      <td>0.179648</td>\n",
       "      <td>-0.371582</td>\n",
       "      <td>...</td>\n",
       "      <td>1.243632</td>\n",
       "      <td>-0.405581</td>\n",
       "      <td>1.085325</td>\n",
       "      <td>1.973006</td>\n",
       "      <td>0.290641</td>\n",
       "      <td>0.963295</td>\n",
       "      <td>-1.359209</td>\n",
       "      <td>2.193898</td>\n",
       "      <td>0.814181</td>\n",
       "      <td>1.235836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.176708</td>\n",
       "      <td>1.625126</td>\n",
       "      <td>-3.843431</td>\n",
       "      <td>0.226879</td>\n",
       "      <td>-0.763457</td>\n",
       "      <td>-0.964958</td>\n",
       "      <td>1.278523</td>\n",
       "      <td>-0.875336</td>\n",
       "      <td>-0.665511</td>\n",
       "      <td>-0.101888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091525</td>\n",
       "      <td>0.851619</td>\n",
       "      <td>0.267266</td>\n",
       "      <td>1.929255</td>\n",
       "      <td>-0.984592</td>\n",
       "      <td>1.801168</td>\n",
       "      <td>-0.403364</td>\n",
       "      <td>0.300297</td>\n",
       "      <td>-1.973638</td>\n",
       "      <td>0.862498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.729852</td>\n",
       "      <td>2.036257</td>\n",
       "      <td>-1.206615</td>\n",
       "      <td>2.121351</td>\n",
       "      <td>-1.688091</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>-0.156384</td>\n",
       "      <td>-1.596992</td>\n",
       "      <td>1.558975</td>\n",
       "      <td>0.324975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383543</td>\n",
       "      <td>0.338034</td>\n",
       "      <td>-0.095333</td>\n",
       "      <td>-0.785435</td>\n",
       "      <td>1.885006</td>\n",
       "      <td>0.938034</td>\n",
       "      <td>-0.616108</td>\n",
       "      <td>-0.222979</td>\n",
       "      <td>1.323855</td>\n",
       "      <td>0.622632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.357656</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.325845</td>\n",
       "      <td>1.146377</td>\n",
       "      <td>-0.941756</td>\n",
       "      <td>0.425242</td>\n",
       "      <td>0.890695</td>\n",
       "      <td>-1.161934</td>\n",
       "      <td>0.509555</td>\n",
       "      <td>0.397377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773701</td>\n",
       "      <td>2.106979</td>\n",
       "      <td>0.112369</td>\n",
       "      <td>1.145097</td>\n",
       "      <td>-0.666539</td>\n",
       "      <td>1.018525</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>1.301296</td>\n",
       "      <td>-0.844504</td>\n",
       "      <td>0.503519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.341450</td>\n",
       "      <td>-0.485298</td>\n",
       "      <td>1.107402</td>\n",
       "      <td>1.558929</td>\n",
       "      <td>-0.446929</td>\n",
       "      <td>-1.098254</td>\n",
       "      <td>-0.614124</td>\n",
       "      <td>0.109389</td>\n",
       "      <td>3.285658</td>\n",
       "      <td>-2.086398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.565137</td>\n",
       "      <td>0.503009</td>\n",
       "      <td>-0.716071</td>\n",
       "      <td>1.805186</td>\n",
       "      <td>-0.334426</td>\n",
       "      <td>0.404759</td>\n",
       "      <td>0.504305</td>\n",
       "      <td>2.839343</td>\n",
       "      <td>0.346494</td>\n",
       "      <td>3.016829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.979038</td>\n",
       "      <td>-1.094407</td>\n",
       "      <td>3.389443</td>\n",
       "      <td>0.096807</td>\n",
       "      <td>0.548192</td>\n",
       "      <td>1.567318</td>\n",
       "      <td>-1.370609</td>\n",
       "      <td>-0.931647</td>\n",
       "      <td>-0.172467</td>\n",
       "      <td>-2.496654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473266</td>\n",
       "      <td>0.977193</td>\n",
       "      <td>-0.845596</td>\n",
       "      <td>0.515236</td>\n",
       "      <td>0.673198</td>\n",
       "      <td>0.404918</td>\n",
       "      <td>-0.761746</td>\n",
       "      <td>-0.420401</td>\n",
       "      <td>0.307847</td>\n",
       "      <td>-0.302959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.624343</td>\n",
       "      <td>-0.447252</td>\n",
       "      <td>1.903704</td>\n",
       "      <td>-0.778280</td>\n",
       "      <td>-0.256622</td>\n",
       "      <td>2.204152</td>\n",
       "      <td>-0.684100</td>\n",
       "      <td>0.221853</td>\n",
       "      <td>0.757391</td>\n",
       "      <td>-1.835602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036426</td>\n",
       "      <td>2.565432</td>\n",
       "      <td>-0.404975</td>\n",
       "      <td>0.285705</td>\n",
       "      <td>-0.626100</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>-1.684269</td>\n",
       "      <td>2.971559</td>\n",
       "      <td>-0.948307</td>\n",
       "      <td>0.275110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.632599</td>\n",
       "      <td>1.512969</td>\n",
       "      <td>0.063073</td>\n",
       "      <td>0.599132</td>\n",
       "      <td>1.569201</td>\n",
       "      <td>1.063324</td>\n",
       "      <td>-1.061380</td>\n",
       "      <td>0.315820</td>\n",
       "      <td>-0.638413</td>\n",
       "      <td>0.355060</td>\n",
       "      <td>...</td>\n",
       "      <td>1.728565</td>\n",
       "      <td>1.498970</td>\n",
       "      <td>0.054502</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.418277</td>\n",
       "      <td>-2.493387</td>\n",
       "      <td>1.780422</td>\n",
       "      <td>1.000546</td>\n",
       "      <td>0.551793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.888098</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>-1.635177</td>\n",
       "      <td>1.314313</td>\n",
       "      <td>2.097363</td>\n",
       "      <td>0.639267</td>\n",
       "      <td>-0.846642</td>\n",
       "      <td>-1.997691</td>\n",
       "      <td>-0.835741</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>3.085684</td>\n",
       "      <td>0.343530</td>\n",
       "      <td>2.113288</td>\n",
       "      <td>1.427790</td>\n",
       "      <td>1.250673</td>\n",
       "      <td>0.566318</td>\n",
       "      <td>2.922274</td>\n",
       "      <td>0.636758</td>\n",
       "      <td>1.900987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.039065</td>\n",
       "      <td>-1.111172</td>\n",
       "      <td>-0.568909</td>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>0.272422</td>\n",
       "      <td>-2.143591</td>\n",
       "      <td>0.038883</td>\n",
       "      <td>0.340247</td>\n",
       "      <td>-0.874415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038609</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>-0.459777</td>\n",
       "      <td>-0.185047</td>\n",
       "      <td>-0.115265</td>\n",
       "      <td>-0.171016</td>\n",
       "      <td>1.042122</td>\n",
       "      <td>2.906338</td>\n",
       "      <td>-0.792142</td>\n",
       "      <td>0.486340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.707917</td>\n",
       "      <td>-0.256485</td>\n",
       "      <td>-0.452564</td>\n",
       "      <td>-0.123952</td>\n",
       "      <td>1.131047</td>\n",
       "      <td>-0.965086</td>\n",
       "      <td>-0.273190</td>\n",
       "      <td>-0.659415</td>\n",
       "      <td>-1.277485</td>\n",
       "      <td>-0.328948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312385</td>\n",
       "      <td>1.883555</td>\n",
       "      <td>1.398873</td>\n",
       "      <td>1.191182</td>\n",
       "      <td>-0.404498</td>\n",
       "      <td>1.748411</td>\n",
       "      <td>0.581175</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>-0.658613</td>\n",
       "      <td>1.504646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 64 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:13:13.664793Z",
     "start_time": "2025-04-21T11:13:13.656719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare Q,K,V\n",
    "# Prepare Query, Key, Value for Multi-head Attention\n",
    "\n",
    "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "# Define Query, Key, Value weight matrices\n",
    "Wq = nn.Linear(d_model, d_model)\n",
    "Wk = nn.Linear(d_model, d_model)\n",
    "Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Wq(query) #[4, 16, 64]\n",
    "Q = Q.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "K = Wk(key) #[4, 16, 64]\n",
    "K = K.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "V = Wv(value) #[4, 16, 64]\n",
    "V = V.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]"
   ],
   "id": "489ee3ba588afc96",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:13:50.308773Z",
     "start_time": "2025-04-21T11:13:50.304294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]\n",
    "# The reason is that treat each batch with \"num_heads\" as its first dimension.\n",
    "Q = Q.transpose(1, 2) # [4, 4, 16, 16]\n",
    "K = K.transpose(1, 2) # [4, 4, 16, 16]\n",
    "V = V.transpose(1, 2) # [4, 4, 16, 16]"
   ],
   "id": "fd55e5c0bc3ffcd0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:14:10.556716Z",
     "start_time": "2025-04-21T11:14:10.552038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate QK^T Attention\n",
    "# Calculate the attention score betwee Q and K^T\n",
    "attention_score = torch.matmul(Q, K.transpose(-2, -1))"
   ],
   "id": "793f306038f6ba1a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:19:11.225852Z",
     "start_time": "2025-04-21T11:19:11.222442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Then Scale the attention score by the square root of the head size\n",
    "attention_score = attention_score / torch.sqrt(torch.tensor(d_model // num_heads, dtype=torch.float))\n",
    "# pd.DataFrame(attention_score[0][0].detach().cpu().numpy())"
   ],
   "id": "ea345f3723d09397",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:19:15.539208Z",
     "start_time": "2025-04-21T11:19:15.534210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply Mask to attention scores\n",
    "attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf')) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "# pd.DataFrame(attention_score[0][0].detach().cpu().numpy())"
   ],
   "id": "ea0c938c76f9baa",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:16:48.474145Z",
     "start_time": "2025-04-21T11:16:48.469988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Softmax the attention score\n",
    "attention_score = torch.softmax(attention_score, dim=-1)  #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]"
   ],
   "id": "df5e3c4ec009ebd1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:17:19.977672Z",
     "start_time": "2025-04-21T11:17:19.971280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the V attention output\n",
    "A = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]\n",
    "A.shape"
   ],
   "id": "edcfae3c6b0e6f1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 16, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:17:31.012258Z",
     "start_time": "2025-04-21T11:17:31.007597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]\n",
    "A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]"
   ],
   "id": "ec7b2b189f9fbf75",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:17:38.758200Z",
     "start_time": "2025-04-21T11:17:38.752709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the output weight matrix\n",
    "Wo = nn.Linear(d_model, d_model)\n",
    "output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "print(output.shape)"
   ],
   "id": "9213cda7490aa34e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:17:59.929872Z",
     "start_time": "2025-04-21T11:17:59.923500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add residual connection\n",
    "output = output + X\n",
    "\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ],
   "id": "201c5204628f6e12",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:18:14.132233Z",
     "start_time": "2025-04-21T11:18:14.121650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Feed Forward Network\n",
    "output = nn.Linear(d_model, d_model * 4)(output)\n",
    "output = nn.ReLU()(output)\n",
    "output = nn.Linear(d_model * 4, d_model)(output)\n",
    "output = torch.dropout(output, p=dropout, train=True)"
   ],
   "id": "d08c8d3ac26f36b8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:18:24.582473Z",
     "start_time": "2025-04-21T11:18:24.577547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add residual connection\n",
    "output = output + X\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ],
   "id": "6221e386e808abfe",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:30:08.559539Z",
     "start_time": "2025-04-21T11:30:08.510944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply final linear layer to get the logits\n",
    "logits = nn.Linear(d_model, max_token_value+1)(output)\n",
    "logits.shape"
   ],
   "id": "b1886d1788753901",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 100070])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:30:33.419795Z",
     "start_time": "2025-04-21T11:30:33.394361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "possibilities = F.softmax(logits, dim=-1)\n",
    "predicted_index = torch.argmax(possibilities[0, 0]).item()\n",
    "print(predicted_index)\n",
    "encoding.decode([predicted_index])"
   ],
   "id": "ce47464dd8f405f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tog'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
